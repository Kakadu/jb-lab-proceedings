\section*{Introduction}\label{sec:introduction}
In the era of big data and high load computations, graphical processing units
 (GPUs) are used extensively for data processing. 
 There have even been designed embedded devices with the support of GPUs
  for general purpose computations, like image recognition on mobile robots~\cite{NVJETSON}.
In practice many GPU-based applications tend to be bandwidth bound, meaning
 that data allocation and access are the main bottlenecks, thus memory optimizations
  appear to be in a prevailing significance and are addressed in a huge amount of research.

While the problem of available GPU memory could be tackled with sophisticated memory pooling techniques
 through memory swapping and sharing~\cite{zhang2019efficient},
  memory architecture of a GPU often requires more sophisticated optimizations.
   Given that a typical GPU incorporates several memory types, each having different
    capacity and throughput, a few memory management automation techniques have been introduced. They could leverage more effective memory spaces to handle register spilling and systematically consider the performance benefit achievable through a specific allocation of shared memory to save global memory transactions~\cite{AutomaticSharedMem,RegisterSpilling}. Further, the diversity of memory types imposes that each memory access should satisfy memory type specific patterns, to be most effective. Under the non-fulfillment of these patterns the data throughput of an application is aggravated due to the increase in the number of required memory transactions. Considering the following typical scenario of a GPU-accelerated application, another runtime memory optimization could be proposed.

 To facilitate the data processing a GPU routine is executed by multiple threads simultaneously with different pieces of data, often exceeding the maximal number of threads
  that could be executed simultaneously resulting in blocks of threads being executed iteratively.
 Being that the input data often exceeds the available GPU memory, the routine could not be applied at once, 
 and there is a need to split the data into chunks and process them iteratively 
 by the routine.
 It happens that some relatively small properties within a processing routine are maintained between the iterations
  and could be considered static in that sense.
 For example, if the application is a GPU-accelerated data processing engine that allows one to write queries to data and typically these queries remain small if compared to data and take significant execution time, the query, once specified by the user, can be used as a static, i.e. already known and constant, data for the query execution kernel runtime optimization since it remains unchanged during the host code execution.
 This observation allows to apply a \textit{partial evaluation} technique to optimize such routines in runtime.

\textit{Partial evaluation} or program \textit{specialization}~\cite{Jones1993,PartialEvalPaper} is a program
 transformation and optimization technique that optimizes a given program with
  respect to statically known inputs, producing another program which,
   if given only the remaining dynamic inputs, will produce the same results
    as initial one would have produced, given both inputs.
Basically, a partial evaluator performs an aggressive unfolding/unrolling, 
inlining, and constant propagation.  
The application of partial evaluation at runtime has 
recently shown a significant performance improvement of query execution for CPU-based
 database querying~\cite{LLVMmix}. 

Regarding memory optimizations of GPU kernels, partial evaluation is 
able to embed static data memory accesses into the code, i.e. place it directly 
into registers, once a kernel is properly written, which could result in a better
 performance compared to non-embedded access since memory transactions would be replaced 
 by accesses to the instruction cache. Thus, the aim of this work is to provide 
 an empirical evaluation of an existing partial evaluator AnyDSL~\cite{LeiBa} 
 that is capable of producing CUDA~\footnote{Programming and hardware model by NVIDIA.} code for NVIDIA GPUs
  to investigate the effects that appear after partially evaluating GPU applications,
   and what aspects of GPU architecture affect the desired result and 
   whether any significant performance improvements could be achieved at all.
   More specifically, the work performs the evaluation on string matching and convolutional filtering scenarios, providing some relevant CUDA assembly examples to ground the effects being observed.
